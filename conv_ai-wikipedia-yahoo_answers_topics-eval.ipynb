{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pandas\n",
    "# !pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sed -i ':a;N;$!ba;s/\\,\\n/\\,/g' results/Cameron*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import analysis_relabel_funcs\n",
    "from analysis import read_dfs\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import chi2_contingency\n",
    "from torch.nn import Softmax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pickle\n",
    "import swifter\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RELABEL = {\n",
    "    'BERT-Jigsaw': [\"not toxic\", \"toxic\"], \n",
    "    'BERT-SBIC-offensive': [\"Not\", \"Maybe\", \"Offensive\"],\n",
    "    'BERT-SBIC-targetcategory': ['none', 'body', 'culture', 'disabled', 'gender', 'race', 'social', 'victim'], # not used\n",
    "    'BERT-eec-emotion': ['none', 'anger','fear', 'joy', 'sadness'], # not used\n",
    "    'BERT-jigsaw-identityhate': ['Not', \"Yes\"],\n",
    "    'BERT-jigsaw-severetoxic': [\"Not\", \"Yes\"],\n",
    "    'BERT-mdgender-convai-binary': [\"female\", \"male\"],\n",
    "    'BERT-mdgender-convai-ternary': [\"female\", \"male\", \"neutral\"],\n",
    "    'BERT-mdgender-wizard': [\"neutral\", \"female\", \"male\"],\n",
    "    'BERT-rtgender-opgender-annotations': [\"man\", \"woman\"] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax_and_relabel(predictions, categorical_labels):\n",
    "    # takes in a torch.Tensor of predictions and a list of categorical labels, returns a tuple of (softmax tensor, categorical label)\n",
    "    m = Softmax(dim=0)\n",
    "    sm = m(predictions)\n",
    "    return sm, categorical_labels[sm.argmax().item()]\n",
    "\n",
    "def score(labels, predictions):\n",
    "    softmax_preds, category = _softmax_and_relabel(predictions, labels)\n",
    "    # scores are always the difference between the first and last\n",
    "    return softmax_preds[-1] - softmax_preds[0], category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOXIC_MODELS = {\"Cameron/BERT-Jigsaw\", \n",
    "                'Cameron/BERT-SBIC-offensive', \n",
    "                #'Cameron/BERT-jigsaw-identityhate', 'Cameron/BERT-jigsaw-severetoxic'\n",
    "               }\n",
    "GENDER_MODELS = {'Cameron/BERT-mdgender-convai-binary', 'Cameron/BERT-mdgender-convai-ternary', 'Cameron/BERT-mdgender-wizard', 'Cameron/BERT-rtgender-opgender-annotations'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "FILES = glob.glob(\"results/Cameron*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FILES[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_files = {}\n",
    "toxic_files = []\n",
    "gender_files = []\n",
    "\n",
    "def create_tup(filename, model_name):\n",
    "    return (model_name.replace(\"Cameron/\", \"\"), filename)\n",
    "\n",
    "for filename in FILES:\n",
    "    s = filename[len(\"results/\"):].split(\":\")\n",
    "    \n",
    "    model_name = s[0][:-3]\n",
    "    model_name = model_name.replace(\"Cameron-\", \"Cameron/\")\n",
    "    eval_dataset_name = s[-1][3:-15]\n",
    "    model_type = None\n",
    "    if eval_dataset_name not in eval_files:\n",
    "        eval_files[eval_dataset_name] = {\"toxic\": [], \"gender\": []}\n",
    "    if model_name in TOXIC_MODELS:\n",
    "        model_type = \"TOXIC\"\n",
    "        eval_files[eval_dataset_name][\"toxic\"].append(create_tup(filename, model_name))\n",
    "        toxic_files.append(filename)\n",
    "    elif model_name in GENDER_MODELS:\n",
    "        model_type = \"GENDER\"\n",
    "        eval_files[eval_dataset_name][\"gender\"].append(create_tup(filename, model_name))\n",
    "        gender_files.append(filename)\n",
    "    else:\n",
    "        print(f\"Model type is not classified, please classify {model_name} with file {filename}\")\n",
    "        continue\n",
    "    print(f\"Model name: {model_name} | eval dataset: {eval_dataset_name}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(toxic_files)} toxic results and {len(gender_files)} gender files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = {}\n",
    "def get_cache(filename):\n",
    "    if filename in CACHE:\n",
    "        return CACHE[filename]\n",
    "    return None\n",
    "\n",
    "def store_cache(filename, data):\n",
    "    CACHE[filename] = data\n",
    "    \n",
    "def cross_tab_gender_toxic(eval_file_split, eval_dataset_name, return_one = False):\n",
    "    global CACHE\n",
    "    CACHE = {}\n",
    "    print(eval_file_split)\n",
    "    eval_crosstabs = []\n",
    "    count = 0.0\n",
    "    tot = len(eval_file_split[\"toxic\"]) * len(eval_file_split[\"gender\"])\n",
    "    print(tot)\n",
    "    for toxic_tup in eval_file_split[\"toxic\"]:\n",
    "        toxic_model_name = toxic_tup[0]\n",
    "        toxic_filename = toxic_tup[1]\n",
    "        for gender_tup in eval_file_split[\"gender\"]:\n",
    "            gender_model_name = gender_tup[0]\n",
    "            gender_filename = gender_tup[1]\n",
    "            \n",
    "            # for using sbc and rt_gender for conv_ai\n",
    "            _, _, combined = read_dfs(toxic_filename, \n",
    "                                      gender_filename,\n",
    "                                     suffixes=(\"_\" + toxic_model_name, \"_\" + gender_model_name))\n",
    "            pred_toxic = \"predictions_\" + toxic_model_name\n",
    "            pred_gender = \"predictions_\" + gender_model_name\n",
    "            \n",
    "            print(\"relabeling toxic\")\n",
    "            s = get_cache(toxic_filename)\n",
    "            if s is None:\n",
    "                print(f\"did not find {toxic_filename} in cache\")\n",
    "                p = partial(score, MODEL_RELABEL[toxic_model_name])\n",
    "                s = combined[pred_toxic].swifter.apply(p)\n",
    "                scores_col = f\"scores_{toxic_model_name}\"\n",
    "                category_col = f\"category_{toxic_model_name}\"\n",
    "            \n",
    "                s = pd.DataFrame(s.tolist(), columns=[scores_col, category_col])\n",
    "                s[scores_col] = s[scores_col].swifter.apply(lambda x: x.item()) # comes back as a tensor, change it to float\n",
    "                store_cache(toxic_filename, s)\n",
    "            else:\n",
    "                print(f\"found toxic {toxic_filename} in cache\")\n",
    "            combined = combined.join(s)\n",
    "            \n",
    "            print(\"relabeling gender\")\n",
    "            s = get_cache(gender_filename)\n",
    "            if s is None:\n",
    "                print(f\"did not find {gender_filename} in cache\")\n",
    "                p = partial(score, MODEL_RELABEL[gender_model_name])\n",
    "                s = combined[pred_gender].swifter.apply(p)\n",
    "                scores_col = f\"scores_{gender_model_name}\"\n",
    "                category_col = f\"category_{gender_model_name}\"\n",
    "            \n",
    "                s = pd.DataFrame(s.tolist(), columns=[scores_col, category_col])\n",
    "                s[scores_col] = s[scores_col].swifter.apply(lambda x: x.item()) # comes back as a tensor, change it to float\n",
    "                store_cache(gender_filename, s)\n",
    "            else:\n",
    "                print(f\"found gender {gender_filename} in cache\")\n",
    "            combined = combined.join(s)\n",
    "            \n",
    "            \n",
    "            eval_crosstabs.append({\n",
    "                \"toxic_model_name\": toxic_model_name,\n",
    "                \"gender_model_name\": gender_model_name,\n",
    "                \"toxic_attr\": (f\"scores_{toxic_model_name}\", f\"category_{toxic_model_name}\"),\n",
    "                \"gender_attr\": (f\"scores_{gender_model_name}\", f\"category_{gender_model_name}\"),\n",
    "                \"df\": combined\n",
    "            })\n",
    "            count += 1.0\n",
    "            print(f\"Crossing {toxic_model_name} x {gender_model_name} for {eval_dataset_name}, finished {count/tot*100}%\")\n",
    "            if return_one:\n",
    "                return eval_crosstabs\n",
    "    return eval_crosstabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebook_files = ['yahoo_answers_topics']\n",
    "print(eval_files['yahoo_answers_topics'])\n",
    "for eval_dataset_name in notebook_files:\n",
    "    eval_f = eval_files[eval_dataset_name]\n",
    "#     if eval_dataset_name != \"conv_ai_3\":\n",
    "#         continue\n",
    "#     if eval_dataset_name == \"air_dialogue\" or eval_dataset_name == \"empathetic_dialogues\":\n",
    "#         print(f'ignoring {eval_dataset_name} for now')\n",
    "#         continue\n",
    "    print(f\"Analyzing {eval_dataset_name}\")\n",
    "    try:\n",
    "        ct = cross_tab_gender_toxic(eval_f, eval_dataset_name, return_one=False)\n",
    "        pickle.dump((eval_dataset_name, ct), open(f\"crosstabs-pickle-{eval_dataset_name}.p\", \"wb\"))\n",
    "        #crosstabs.append((eval_dataset_name, ct))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
